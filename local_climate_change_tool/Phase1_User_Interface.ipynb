{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting Started:**\n",
    "Before running the [climate dashboard](phase2_dashboard_generator/climate_dashboard.ipynb), it is necessary to acquire a set of processed climate model and observation datasets (which are generated by the phase 1 data wrangler; more detail provided in the [component specification](../docs/Component_Specification.pdf)). Users can acquire such data in two ways:\n",
    "\n",
    "### Option A. Downloading processed climate model and observation datasets\n",
    "***This is the best option for most of our target users***, as it allows you to launch the dashboard without navigating a high performance computing system. In this option, you download datafiles that have already been generated by running the data wrangler (current as of December 2, 2019). This works on any platform (your local computer, or high-performance clusters such as ocean.pangeo.io).\n",
    "* **Pros:** Works across all computing systems, avoids using large datasets that exceed the memory requirements of most personal computers\n",
    "* **Cons:** Will not have the most up-to-date climate model and observation data, as it requires the development team to run the data wrangler and upload output as new data becomes available.\n",
    "\n",
    "### Option B. Running the data wrangler module yourself\n",
    "This is the best option for users with some technical background and ability to interact with a high-performance computing system (*note:* anyone who has an ORCID can run this on the geosciences computing cluster such as ocean.pangeo.io). Because raw climate model data is very large, loading datasets and doing calculations across datasets (like calculating the mean across models) is not feasible on most desktops.\n",
    "* **Pros:** Running the data wrangler will automatically incorporate the most up-to-date  datasets on the google-cloud-based Coupled Model Intercomparison Project Phase 6 (CMIP6) data archive, so this option will incorporate the most up-to-date climate model and observation data into the dashboard\n",
    "* **Cons:** Requires access to a high-performance computing system, generates intermediate datafiles that are large in size, takes about 30 minutes to run the data wrangler\n",
    "\n",
    "#### Below, we provide an interface through which users can proceed through either of these two options. To set up your workspace to run the [climate dashboard](phase2_dashboard_generator/climate_dashboard.ipynb), run step 1 below and then either step 2A or 2B, depending on your preference given the option descriptions above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Set up workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NOTE:** Running this notebook requires your environment to have *oauth2client* and *tarfile* installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_file_from_google_drive as import_files\n",
    "print_statements_on = True #if True, this will print progress updates as files are downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download raw historical observation dataset (that needs to be processed)\n",
    "If you have any issues, you can manually download the data from: https://drive.google.com/drive/folders/1m_NgSTKQB9KFa6mdpdFvPdc2Wr7xemTC?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> getting credentials\n",
      " -> downloading files\n"
     ]
    }
   ],
   "source": [
    "import_files.download_data_predefined('Raw_Historical_Obs', print_statements_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download smaller datafiles that can be used for testing\n",
    "If you have any issues, you can manually download the data from: https://drive.google.com/drive/folders/1eyf9f2rlokSxB5WeP25JHE9YEewjGCaZ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_files.download_data_predefined('Files_for_Testing', print_statements_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2, Option A:** Download processed climate model and observation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads processed data from google drive. This works on any platform (ocean.pangeo.io and your local computer).\n",
    "If you have any issues, you can manually download the data from: https://drive.google.com/open?id=1M9XhNtSiDJsgK0-6mXmM-lR9JGyEakqI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_files.download_data_predefined('Processed_Data', print_statements_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2, Option B:** Run the data wrangler module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running subcomponent A---------------\n",
      "====> Creating data dictionary of available model data\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 67 group(s)\n"
     ]
    }
   ],
   "source": [
    "# Import data wrangler module\n",
    "from phase1_data_wrangler import data_wrangler\n",
    "\n",
    "# Run data_wrangler (this links all the subcomponents in the appropriate order)\n",
    "# Note that this could take up to an hour.\n",
    "data_wrangler.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
