{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_subcomp_a as testing\n",
    "import datetime\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subcomp_a_process_climate_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = analysis_parameters.DIR_INTERMEDIATE_PROCESSED_MODEL_DATA\n",
    "scenario='ssp126'\n",
    "normalized = False\n",
    "WORKING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6f4a4004dcca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_scenario_fnames(data_path, scenario, normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tas_ssp126_MRI-ESM2-0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module processes raw CMIP6 output to create files with consistent dimensions for all models.\n",
    "\n",
    "Running this script for all models with just averaging over ensembles, regridding,\n",
    "and saving takes about 15-20 mins.\n",
    "\n",
    "Make sure to clear the directory you would like the zarr files to be saved in - something about\n",
    "this format makes it not work to overwrite existing files.\n",
    "\"\"\"\n",
    "\n",
    "####################### Load Packages ########################################\n",
    "\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import analysis_parameters\n",
    "\n",
    "############ Read in Settings for Data Dictionary#############################\n",
    "\n",
    "THIS_EXPERIMENT_ID = analysis_parameters.EXPERIMENT_LIST\n",
    "THIS_VARIABLE_ID = analysis_parameters.VARIABLE_ID\n",
    "OUTPUT_PATH = analysis_parameters.DIR_PROCESSED_DATA\n",
    "DIR_INTERMEDIATE = analysis_parameters.DIR_INTERMEDIATE_PROCESSED_MODEL_DATA\n",
    "\n",
    "########### Create functions for analysis ####################################\n",
    "\n",
    "def create_reference_grid(reference_key, dset_dict):\n",
    "    \"\"\"Creates reference grid to which all model output will be regridded\"\"\"\n",
    "    thisdata = dset_dict[reference_key]\n",
    "    ds_out = xr.Dataset({'lat': thisdata['lat'],\n",
    "                         'lon': thisdata['lon']})\n",
    "    return ds_out\n",
    "\n",
    "def reindex_time(startingtimes):\n",
    "    \"\"\"Reindexes time series to proleptic Gregorian calendar type\"\"\"\n",
    "    newtimes = np.empty(np.shape(startingtimes.values),dtype=cftime.DatetimeProlepticGregorian)\n",
    "    for i in range(0, len(startingtimes)):\n",
    "        yr = int(str(startingtimes.values[i])[0:4])\n",
    "        mon = int(str(startingtimes.values[i])[5:7])\n",
    "        newdate = cftime.DatetimeProlepticGregorian(yr, mon, 15)\n",
    "        newtimes[i] = newdate\n",
    "    return newtimes\n",
    "\n",
    "def regrid_model(ds, reference_grid, latvariable='lat',\n",
    "                 lonvariable='lon', regrid_method='nearest_s2d'):\n",
    "    \"\"\"Regrids model output to a reference grid\"\"\"\n",
    "    data_series = ds[THIS_VARIABLE_ID]\n",
    "    ds_in = xr.Dataset({'lat': data_series[latvariable],\n",
    "                        'lon': data_series[lonvariable],\n",
    "                        'time': data_series['time'],\n",
    "                        THIS_VARIABLE_ID: data_series})\n",
    "    regridder = xe.Regridder(ds_in, reference_grid, regrid_method, periodic=True)\n",
    "    data_series_regridded = regridder(ds_in)\n",
    "    data_series_regridded.attrs.update(data_series.attrs)\n",
    "    return data_series_regridded\n",
    "\n",
    "def process_dataset(this_key, dset_dict, final_grid):\n",
    "    \"\"\" Processes each dataset in dictionary\n",
    "    This processing involves:\n",
    "        (1) Averaging over all ensemble members (member ids)\n",
    "        (2) Getting into consistent time format\n",
    "        (3) Renaming coordinates if necessary\n",
    "        (4) Regridding to reference dataset\n",
    "    \"\"\"\n",
    "    # Get original dataset from dictionary\n",
    "    ds_original = dset_dict[this_key]\n",
    "\n",
    "    # Average over all ensemble members\n",
    "    ds = ds_original.mean(dim=['member_id'])\n",
    "\n",
    "    # Reindex time to consistent time datatype\n",
    "    ds = xr.decode_cf(ds)\n",
    "    newtimes = reindex_time(startingtimes=ds['time'])\n",
    "    ds['time'] = xr.DataArray(newtimes, coords=[newtimes], dims=['time'])\n",
    "    ds = ds.copy()\n",
    "\n",
    "    # Rename latitude and longitude coordinate names if necessary\n",
    "    if 'latitude' in ds.dims:\n",
    "        ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Regrid dataset to reference grid\n",
    "    dataset_regridded = regrid_model(ds, final_grid)\n",
    "\n",
    "    return dataset_regridded\n",
    "\n",
    "def generate_new_filename(this_key):\n",
    "    \"\"\"Generates filename for processed data\"\"\"\n",
    "    [_, _, source_id, experiment_id, _, _] = this_key.split('.')\n",
    "    this_fname = THIS_VARIABLE_ID+'_'+experiment_id+'_'+source_id\n",
    "    return this_fname\n",
    "\n",
    "##################### Main Workflow ##########################################\n",
    "\n",
    "def process_all_files_in_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests for subcomponent a\n",
    "\"\"\"\n",
    "\n",
    "# Import modules\n",
    "import datetime\n",
    "import cftime\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "#import analysis_parameters\n",
    "#import subcomp_a_create_data_dict\n",
    "import subcomp_a_process_climate_model_data as process_data\n",
    "\n",
    "# Define directory and file names\n",
    "TEST_KEY1 = 'ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.Amon.gn'\n",
    "TEST_KEY2 = 'CMIP.CAMS.CAMS-CSM1-0.historical.Amon.gn'\n",
    "\n",
    "TESTING_DATA_DIR = '/home/jovyan/local-climate-data-tool/Data/files_for_testing/raw_data/'\n",
    "TESTING_OUTPUT_DIR = '/home/jovyan/local-climate-data-tool/Data/files_for_testing/processed_model_data/'\n",
    "VARNAME = 'tas'\n",
    "TEST_INDS = [44, 120, 0]\n",
    "EXP_TYPES = np.array([xr.core.dataarray.DataArray,\n",
    "                      np.ndarray,\n",
    "                      np.float64,\n",
    "                      cftime._cftime.DatetimeProlepticGregorian])\n",
    "\n",
    "# Read testing data into dictionary\n",
    "DSET_DICT = dict()\n",
    "DSET_DICT[TEST_KEY1] = xr.open_dataset(TESTING_DATA_DIR+TEST_KEY1+'.nc')\n",
    "DSET_DICT[TEST_KEY2] = xr.open_dataset(TESTING_DATA_DIR+TEST_KEY2+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite existing file: nearest_s2d_144x192_160x320_peri.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "using dimensions ('lat', 'lon') from data variable tas as the horizontal dimensions for this dataset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "path '' contains a group",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-718e5ab6e76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Export intermediate processed dataset as zarr file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#ds_processed.chunk({'lon':10, 'lat':10, 'time':-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mds_processed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESTING_OUTPUT_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test4'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.zarr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mds_read_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESTING_OUTPUT_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test4'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.zarr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(self, store, mode, synchronizer, group, encoding, compute, consolidated, append_dim)\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m             \u001b[0mconsolidated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsolidated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m             \u001b[0mappend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1617\u001b[0m         )\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(dataset, store, mode, synchronizer, group, encoding, compute, consolidated, append_dim)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0msynchronizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m         \u001b[0mconsolidate_on_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsolidated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     \u001b[0mzstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mzarr_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mzarr_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzarr_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate_on_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/zarr/hierarchy.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0merr_contains_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcontains_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0merr_contains_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0minit_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/zarr/errors.py\u001b[0m in \u001b[0;36merr_contains_group\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0merr_contains_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path %r contains a group'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: path '' contains a group"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "ds_original = DSET_DICT[TEST_KEY1]\n",
    "ds_ref_grid = process_data.create_reference_grid(dset_dict=DSET_DICT,\n",
    "                                                 reference_key=TEST_KEY2)\n",
    "ds_processed = process_data.process_dataset(this_key=TEST_KEY1,\n",
    "                                            dset_dict=DSET_DICT,\n",
    "                                            final_grid=ds_ref_grid)\n",
    "ds_processed.load()\n",
    "ds_processed=ds_processed.copy(deep=True)\n",
    "    # Export intermediate processed dataset as zarr file\n",
    "#ds_processed.chunk({'lon':10, 'lat':10, 'time':-1})\n",
    "ds_processed.to_zarr(TESTING_OUTPUT_DIR+'test4'+'.zarr')\n",
    "ds_read_in = xr.open_zarr(TESTING_OUTPUT_DIR+'test4'+'.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed.to_netcdf(TESTING_OUTPUT_DIR+'test4'+'.nc')\n",
    "ds_read_in = xr.open_dataset(TESTING_OUTPUT_DIR+'test4'+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e2854f56559c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_processed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_read_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(type([0]).year))\n",
    "print(type(ds_read_in['time'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015,\n",
       "            ...\n",
       "            2100, 2100, 2100, 2100, 2100, 2100, 2100, 2100, 2100, 2100],\n",
       "           dtype='int64', length=1032)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------\n",
    "def test_reindex_time():\n",
    "    \"\"\"Tests whether the reindex_time function works as expected\"\"\"\n",
    "    [yr, month, day] = [1850, 1, 15]\n",
    "    expected_times =xr.DataArray(np.array([cftime.DatetimeProlepticGregorian(yr, month, day, 0, 0),\n",
    "                                           cftime.DatetimeProlepticGregorian(yr, month+1, day, 0, 0)]))\n",
    "    testdates1 = xr.DataArray(np.array([np.datetime64(datetime.datetime(yr, month, day, 0, 0)),\n",
    "                                        np.datetime64(datetime.datetime(yr, month+1, day, 0, 0))]))\n",
    "    testdates2 = xr.DataArray(np.array([cftime.DatetimeNoLeap(yr, month, day, 0, 0),\n",
    "                                        cftime.DatetimeNoLeap(yr, month+1, day, 0, 0)]))\n",
    "    testdates3 = xr.DataArray(np.array([cftime.Datetime360Day(yr, month, day, 0, 0),\n",
    "                                        cftime.Datetime360Day(yr, month+1, day, 0, 0)]))\n",
    "    dates_to_test = [testdates1, testdates2, testdates3]\n",
    "\n",
    "    # Run test\n",
    "    dates_converted = True\n",
    "    for date_to_test in dates_to_test:\n",
    "        newtimes = process_data.reindex_time(date_to_test)\n",
    "        if not np.all((newtimes==expected_times).values):\n",
    "            dates_converted = False\n",
    "        else:\n",
    "            continue\n",
    "    assert dates_converted\n",
    "\n",
    "def test_generate_new_filename(test_key=TEST_KEY1):\n",
    "    \"\"\"Test that the generate_new_filename function generates a string\"\"\"\n",
    "    fname = process_data.generate_new_filename(test_key)\n",
    "    assert ((fname is not None) and (type(fname) == str))\n",
    "\n",
    "def test_create_reference_grid(dset_dict=DSET_DICT, test_key=TEST_KEY2):\n",
    "    \"\"\" Tests that the reference grid function creates a sensible reference grid\"\"\"\n",
    "    ds_original = dset_dict[test_key]\n",
    "    ds_ref_grid = process_data.create_reference_grid(dset_dict=dset_dict,\n",
    "                                                     reference_key=test_key)\n",
    "\n",
    "    # Check that latitude and longitude dimensions haven't changed in regridding\n",
    "    correct_dim_size = ((ds_original.dims['lat'] == ds_ref_grid.dims['lat']) and\n",
    "                        (ds_original.dims['lon'] == ds_ref_grid.dims['lon']))\n",
    "\n",
    "    # Check that reference grid doesn't have any variables other than lat and lon\n",
    "    no_extra_vars = (len(ds_ref_grid.variables) == 2)\n",
    "\n",
    "    # Check that latitude and longitude coordinates do not contain nans\n",
    "    no_nans = ((not np.isnan(ds_ref_grid['lat'].values).any()) and\n",
    "               (not np.isnan(ds_ref_grid['lon'].values).any()))\n",
    "\n",
    "    #Check that latitude and longitude arrays are 1D\n",
    "    one_dim = ((np.size(ds_ref_grid['lat'].values) == np.shape(ds_ref_grid['lat'].values)[0]) and\n",
    "               (np.size(ds_ref_grid['lon'].values) == np.shape(ds_ref_grid['lon'].values)[0]))\n",
    "\n",
    "    # Check that latitude and longitude arrays are big enough\n",
    "    min_coord_size = 20\n",
    "    big_enough = ((np.size(ds_ref_grid['lat'].values) > min_coord_size) and\n",
    "                  (np.size(ds_ref_grid['lon'].values) > min_coord_size))\n",
    "\n",
    "    assert correct_dim_size\n",
    "    assert no_extra_vars\n",
    "    assert no_nans\n",
    "    assert one_dim\n",
    "    assert big_enough\n",
    "\n",
    "def test_regrid_model_dims(dset_dict=DSET_DICT, key_to_regrid=TEST_KEY1,\n",
    "                           key_for_grid=TEST_KEY2):\n",
    "    \"\"\"Tests that the regrid function results in an array of the right dimensions\"\"\"\n",
    "    \n",
    "    # Regrid model output\n",
    "    ds_to_regrid = dset_dict[key_to_regrid]\n",
    "    reference_grid = process_data.create_reference_grid(dset_dict=dset_dict,\n",
    "                                                        reference_key=key_for_grid)\n",
    "    ds_regridded = process_data.regrid_model(ds_to_regrid,\n",
    "                                             reference_grid)\n",
    "    \n",
    "    # Check that lat/lonregridded data actually aligns with reference grid\n",
    "    correct_lat_dim = np.size(ds_regridded['lat'].values) == np.size(reference_grid['lat'].values)\n",
    "    correct_lon_dim = np.size(ds_regridded['lon'].values) == np.size(reference_grid['lon'].values)\n",
    "    # Check that time dimensions have not changed\n",
    "    correct_time_dim = np.equal(ds_regridded['time'].values,ds_to_regrid['time'].values).all()\n",
    "    assert (correct_lat_dim and correct_lon_dim and correct_time_dim)\n",
    "\n",
    "def test_regrid_model_nans(dset_dict=DSET_DICT, key_to_regrid=TEST_KEY1,\n",
    "                           key_for_grid=TEST_KEY2, varname=VARNAME):\n",
    "    \"\"\"Tests that the regrid function doesn't create any nans\"\"\"\n",
    "    ds_to_regrid = dset_dict[key_to_regrid]\n",
    "    reference_grid = process_data.create_reference_grid(dset_dict=dset_dict,\n",
    "                                                        reference_key=key_for_grid)\n",
    "    ds_regridded = process_data.regrid_model(ds_to_regrid,\n",
    "                                             reference_grid)\n",
    "\n",
    "    # Check that there aren't nans if there weren't in original array\n",
    "    if np.isnan(ds_to_regrid[varname].values).any():\n",
    "        raise ValueError('NANs were in original array')\n",
    "    else: \n",
    "        assert not np.isnan(ds_regridded[varname].values).any()\n",
    "\n",
    "def test_process_dataset(dset_dict=DSET_DICT,\n",
    "                         key_to_process=TEST_KEY1,\n",
    "                         key_for_grid=TEST_KEY2,\n",
    "                         test_inds=TEST_INDS,\n",
    "                         expected_types=EXP_TYPES):\n",
    "    \"\"\"Tests whether the processed dataset output by the process_dataset\n",
    "    function has expected coordinates and dimensions and plausible values\"\"\"\n",
    "    #------------------ Process test dataset------------------------------------\n",
    "    ds_original = dset_dict[key_to_process]\n",
    "    ds_ref_grid = process_data.create_reference_grid(dset_dict=dset_dict,\n",
    "                                                     reference_key=key_for_grid)\n",
    "    ds_processed = process_data.process_dataset(this_key=key_to_process,\n",
    "                                                dset_dict=dset_dict,\n",
    "                                                final_grid=ds_ref_grid)\n",
    "\n",
    "    #------------------ Run tests on processed dataset-------------------------\n",
    "    # Check that lat/lon/time coordinate names are right\n",
    "    coord_names_pass = check_coord_names(ds_processed, ds_coords_expected=['time', 'lon', 'lat'])\n",
    "\n",
    "    # Check that time is reasonable (e.g. )\n",
    "    years_pass = check_years(ds_processed, min_year=1849, max_year=2200)\n",
    "\n",
    "    # Check that lat, lon, and time dimensions are what we expect\n",
    "    dims_pass = check_dims(ds_processed, ds_original, ds_ref_grid)\n",
    "\n",
    "    # Check that coordinate types are right\n",
    "    coord_types_pass = check_coord_types(ds_processed, expected_types)\n",
    "\n",
    "    assert coord_names_pass\n",
    "    assert years_pass\n",
    "    assert dims_pass\n",
    "    assert coord_types_pass\n",
    "\n",
    "def test_save_dataset(dset_dict=DSET_DICT,\n",
    "                      data_path_out=TESTING_OUTPUT_DIR,\n",
    "                      key_for_grid=TEST_KEY2,\n",
    "                      exceptions_list=(),\n",
    "                     expected_types=EXP_TYPES):\n",
    "    final_grid = process_data.create_reference_grid(dset_dict=dset_dict,\n",
    "                                                    reference_key=key_for_grid)\n",
    "    \n",
    "    delete_zarr_files(data_dir=data_path_out, regex='*')\n",
    "    \n",
    "    process_data.process_all_files_in_dictionary(dset_dict, exceptions_list,\n",
    "                                        final_grid, data_path_out)\n",
    "    endcut = -1*len('.zarr')\n",
    "    begcut = len(data_path_out)\n",
    "    names = [f[begcut:endcut] for f in glob.glob(data_path_out +'*.zarr')]\n",
    "    files_are_saved = bool(len(names) > 0)\n",
    "    \n",
    "    coord_names_pass = True\n",
    "    years_pass = True\n",
    "    coord_types_pass = True\n",
    "    for fname in names:\n",
    "        filename = data_path_out + fname + '.zarr'\n",
    "        ds = xr.open_zarr(filename)\n",
    "        ds = xr.decode_cf(ds)\n",
    "        \n",
    "        # Check that ds looks the same as the processed dataset\n",
    "        coord_names_pass = coord_names_pass and check_coord_names(ds, ds_coords_expected=['time', 'lon', 'lat'])\n",
    "        coord_types_pass = coord_types_pass and check_coord_types(ds, expected_types)\n",
    "        years_pass = years_pass and check_years(ds, min_year=1849, max_year=2200)\n",
    "    \n",
    "    assert files_are_saved\n",
    "    assert coord_names_pass\n",
    "    assert years_pass\n",
    "    assert coord_types_pass\n",
    "\n",
    "def check_coord_names(ds_processed, ds_coords_expected):\n",
    "    \"\"\"Checks whether coordinate names of ds are expected names\"\"\"\n",
    "    coords_list = []\n",
    "    for coord in ds_processed.coords:\n",
    "        coords_list.append(coord)\n",
    "    return bool(set(coords_list) == set(ds_coords_expected))\n",
    "\n",
    "def check_dims(ds_processed, ds_original, ds_ref_grid):\n",
    "    \"\"\"Checks whether dimensions of dataset are expected based on\n",
    "    regridding and original dataset dimensions\"\"\"\n",
    "    if ds_processed.dims['time'] == ds_original.dims['time']:\n",
    "        if  ds_processed.dims['lat'] == ds_ref_grid.dims['lat']:\n",
    "            if ds_processed.dims['lon'] == ds_ref_grid.dims['lon']:\n",
    "                return True\n",
    "            else:\n",
    "                print('Incorrect lon dimension')\n",
    "                return False\n",
    "        else:\n",
    "            print('Incorrect lat dimension')\n",
    "            return False\n",
    "    else:\n",
    "        print('Incorrect time dimension')\n",
    "        return False\n",
    "\n",
    "def check_years(ds_processed, min_year, max_year):\n",
    "    \"\"\" Check that times are within range of plausible years for\n",
    "    model output\"\"\"\n",
    "    if ds_processed['time'].values[0].year > min_year:\n",
    "        if ds_processed['time'].values[0].year < max_year:\n",
    "            return True\n",
    "        else:\n",
    "            print('Start year is too big')\n",
    "            return False\n",
    "    else:\n",
    "        print('Start year is too small')\n",
    "        return False\n",
    "\n",
    "def check_coord_types(ds_processed, expected_types):\n",
    "    \"\"\"Checks that processed dataset consists of coordinates\n",
    "    of expected data types\"\"\"\n",
    "    [exp_type_dim, exp_type_dim_value, exp_type_latlon, exp_type_time] = expected_types\n",
    "\n",
    "    time_types_pass = (isinstance(ds_processed['time'].values[0], exp_type_time) and\n",
    "                       isinstance(ds_processed['time'], exp_type_dim) and\n",
    "                       isinstance(ds_processed['time'].values, exp_type_dim_value))\n",
    "\n",
    "    lat_types_pass = (isinstance(ds_processed['lat'].values[0], exp_type_latlon) and\n",
    "                      isinstance(ds_processed['lat'], exp_type_dim) and\n",
    "                      isinstance(ds_processed['lat'].values, exp_type_dim_value))\n",
    "\n",
    "    lon_types_pass = (isinstance(ds_processed['lon'].values[0], exp_type_latlon) and\n",
    "                      isinstance(ds_processed['lon'], exp_type_dim) and\n",
    "                      isinstance(ds_processed['lon'].values, exp_type_dim_value))\n",
    "\n",
    "    return bool(time_types_pass and lat_types_pass and lon_types_pass)\n",
    "\n",
    "def delete_zarr_files(data_dir, regex):\n",
    "    \"\"\"Deletes zarr files matching regular expression. This is a\n",
    "    necessary function because zarr files cannot be overwritten\"\"\"\n",
    "    i = 0\n",
    "    for file in glob.glob(data_dir+regex+'.zarr'):\n",
    "        os.system('rm -rf '+file)\n",
    "        i = i+1\n",
    "    print('deleted '+str(i)+' files in '+data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
