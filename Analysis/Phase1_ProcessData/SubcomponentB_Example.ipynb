{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis_parameters\n",
    "import xarray as xr\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multimodelstats as mms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = analysis_parameters.DIR_INTERMEDIATE_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relevant filenames and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_fnames(data_path, scenario):\n",
    "    \"\"\"\n",
    "    Get a string list of all zarr files in the data_path for the given \n",
    "    scenario. Prints the list to the user.\n",
    "    \"\"\"\n",
    "    endcut = -1*len('.zarr')\n",
    "    begcut = len(data_path)\n",
    "    names = [f[begcut:endcut] for f in glob.glob(data_path + '*_' + scenario + '_*.zarr')]\n",
    "    return names\n",
    "\n",
    "def read_in_fname(data_path, fname):\n",
    "    \"\"\"Read in zarr file with name datapath/fname.zarr and return the correpsonding xarray\"\"\"\n",
    "    filename = data_path + fname + '.zarr'\n",
    "    return xr.open_zarr(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_name = 'historical'\n",
    "file_names_hist = get_scenario_fnames(data_path, scenario_name)\n",
    "datasets_hist = [read_in_fname(data_path, fname) for fname in file_names_hist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset\n",
    "\n",
    "Currently a hacky way to make empty datasets of the right dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ds_fname = 'tas_historical_CAMS-CSM1-0'\n",
    "variable_name = 'tas'\n",
    "#def initialize_dataset(data_path, initial_ds_fname):\n",
    "ds_init = read_in_fname(data_path, initial_ds_fname)\n",
    "ds_init.load()\n",
    "\n",
    "ds_dims = np.shape(ds_init[variable_name].values)\n",
    "lats = ds_init['lat'].values\n",
    "lons = ds_init['lon'].values\n",
    "times = ds_init['time']\n",
    "empty_array = np.empty(ds_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.copy(empty_array)\n",
    "max_vals = np.copy(empty_array)\n",
    "min_vals = np.copy(empty_array)\n",
    "std_vals = np.copy(empty_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through lat/lons and calculate for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This currently takes ~7-8 mins to run for each latitude band (160 total), which means it will take about 21 hours to run for all***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "472.96125960350037\n",
      "1\n",
      "915.7842659950256\n",
      "2\n",
      "1367.3400619029999\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(0,len(lats)):\n",
    "    lt = lats[i]\n",
    "    print(i)\n",
    "    for j in range(0,len(lons)):\n",
    "        ln = lons[j]\n",
    "        stats_single_point = mms.export_stats(datasets_hist, file_names_hist, is_global_mean=False, coords=[lt, ln])\n",
    "        mean_vals[:,i,j] = stats_single_point['multi_mean'].values\n",
    "        min_vals[:,i,j] = stats_single_point['multi_min'].values\n",
    "        max_vals[:,i,j] = stats_single_point['multi_max'].values\n",
    "        std_vals[:,i,j] = stats_single_point['multi_std'].values\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have to stop running this partway through, save your progress to pickle\n",
    "output_path_temp = '/home/jovyan/local-climate-data-tool/Data/IntermediateData/'\n",
    "fname_temp = 'temporary_multi_model_stats_grid'\n",
    "\n",
    "with open(output_path_temp+fname_temp+'mean'+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(mean_vals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(output_path_temp+fname_temp+'min'+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(min_vals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(output_path_temp+fname_temp+'max'+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(max_vals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(output_path_temp+fname_temp+'std'+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(std_vals, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Started running at 11:15 am, done with 0-1 by 11:25 am\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empty_array', 811008128),\n",
       " ('max_vals', 811008128),\n",
       " ('mean_vals', 811008128),\n",
       " ('min_vals', 811008128),\n",
       " ('std_vals', 811008128),\n",
       " ('datasets_hist', 264),\n",
       " ('file_names_hist', 264),\n",
       " ('get_scenario_fnames', 136),\n",
       " ('read_in_fname', 136),\n",
       " ('ds_init', 112),\n",
       " ('data_path', 108),\n",
       " ('lats', 96),\n",
       " ('lons', 96),\n",
       " ('times', 96),\n",
       " ('mms', 80),\n",
       " ('np', 80),\n",
       " ('plt', 80),\n",
       " ('xr', 80),\n",
       " ('initial_ds_fname', 75),\n",
       " ('ds_dims', 72),\n",
       " ('scenario_name', 59),\n",
       " ('variable_name', 52)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xarrays from numpy array\n",
    "ds_coords = {'time': ds_init['time'], 'lat': ds_init['lat'], 'lon': ds_init['lon']}\n",
    "ds_dims = ('time', 'lat', 'lon')\n",
    "mean_xr = xr.DataArray(mean_vals, dims=ds_dims, coords=ds_coords)\n",
    "\n",
    "max_xr = xr.DataArray(max_vals, dims=ds_dims, coords=ds_coords)\n",
    "\n",
    "min_xr = xr.DataArray(min_vals, dims=ds_dims, coords=ds_coords)\n",
    "\n",
    "std_xr = xr.DataArray(std_vals, dims=ds_dims, coords=ds_coords)\n",
    "\n",
    "# Create xarray dataset from xarrays\n",
    "ds = xr.Dataset({'mean': mean_xr,\n",
    "                 'min': min_xr,\n",
    "                 'max': max_xr,\n",
    "                 'std': std_xr,},\n",
    "                coords=ds_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset to zarr\n",
    "def export_dataset(ds, output_path, variable_name, scenario_name):\n",
    "    ds.chunk({'lon':10, 'lat':10, 'time':-1})\n",
    "    ds.to_zarr(output_path+'modelData_'+variable_name+'_'+scenario_name+'.zarr')\n",
    "\n",
    "export_dataset(ds=DS,\n",
    "               output_path='/home/jovyan/local-climate-data-tool/Data/ProcessedData/',\n",
    "               variable_name=variable_name,\n",
    "               scenario_name=scenario_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
